/**
 * Agentic Creator OS - Quality Evaluation MCP Server
 * 
 * This MCP server provides AI-powered evaluation and quality assurance
 * for content generated by Agentic Creator OS workflows.
 * 
 * Features:
 * - Content quality scoring (1-100)
 * - Brand voice consistency checking
 * - Platform optimization validation
 * - Engagement prediction
 * - Iterative improvement suggestions
 * - Performance tracking across time
 */

import { Server } from '@modelcontextprotocol/sdk/server/index.js';
import { StdioServerTransport } from '@modelcontextprotocol/sdk/server/stdio.js';
import {
  CallToolRequestSchema,
  ListToolsRequestSchema,
} from '@modelcontextprotocol/sdk/types.js';
import { z } from 'zod';
import { evaluateContent } from './evaluation/evaluator.js';
import { evaluateHook } from './evaluation/hook-evaluator.js';
import { trackPerformance, getMetrics } from './evaluation/metrics.js';
import { logEvaluation, logSession, getAuditTrail } from './logging/audit.js';
import type { EvaluationResult, ContentMetrics, AuditEntry } from './types/index.js';

// Initialize the MCP Server
const server = new Server({
  name: 'agentic-creator-os/evaluator',
  version: '1.0.0',
});

// Tool Schemas
const EvaluateContentSchema = z.object({
  content: z.string(),
  contentType: z.enum(['twitter', 'linkedin', 'instagram', 'tiktok', 'farcaster', 'blog', 'email', 'video']),
  platform: z.string().optional(),
  brandVoice: z.object({
    tone: z.enum(['professional', 'casual', 'authoritative', 'friendly', ' provocative']),
    personality: z.array(z.string()),
    keywords: z.array(z.string()).optional(),
    avoidKeywords: z.array(z.string()).optional(),
  }).optional(),
  context: z.object({
    workflowId: z.string().optional(),
    sessionId: z.string().optional(),
    projectId: z.string().optional(),
    source: z.enum(['claude-code', 'opencode', 'gemini', 'custom']).default('claude-code'),
    timestamp: z.string().optional(),
  }).optional(),
  options: z.object({
    includeSuggestions: z.boolean().default(true),
    strictMode: z.boolean().default(false),
    benchmarkAgainst: z.enum(['industry-average', 'top-performers', 'previous-outputs']).default('industry-average'),
  }).optional(),
});

const EvaluateHookSchema = z.object({
  hook: z.string(),
  platform: z.enum(['twitter', 'linkedin', 'instagram', 'tiktok', 'farcaster']),
  category: z.enum(['question', 'statement', 'contradiction', 'promise', 'story', 'controversy']).optional(),
  context: z.object({
    workflowId: z.string().optional(),
    sessionId: z.string().optional(),
    projectId: z.string().optional(),
    source: z.enum(['claude-code', 'opencode', 'gemini', 'custom']).default('claude-code'),
  }).optional(),
});

const TrackPerformanceSchema = z.object({
  evaluationId: z.string(),
  actualMetrics: z.object({
    impressions: z.number().optional(),
    engagement: z.number().optional(),
    shares: z.number().optional(),
    comments: z.number().optional(),
    saves: z.number().optional(),
    clicks: z.number().optional(),
    conversions: z.number().optional(),
    reach: z.number().optional(),
    followerGrowth: z.number().optional(),
  }),
  publishedAt: z.string(),
  platform: z.string(),
});

const GetMetricsSchema = z.object({
  timeRange: z.enum(['24h', '7d', '30d', '90d', 'all']).default('30d'),
  contentType: z.string().optional(),
  platform: z.string().optional(),
  projectId: z.string().optional(),
  metricType: z.enum(['quality', 'engagement', 'improvement', 'all']).default('all'),
});

const GetAuditTrailSchema = z.object({
  sessionId: z.string().optional(),
  projectId: z.string().optional(),
  source: z.enum(['claude-code', 'opencode', 'gemini', 'all']).optional(),
  startDate: z.string().optional(),
  endDate: z.string().optional(),
  limit: z.number().max(1000).default(100),
});

const CompareContentSchema = z.object({
  contentA: z.string(),
  contentB: z.string(),
  contentType: z.string(),
  metrics: z.array(z.enum(['quality', 'readability', 'engagement', 'seo', 'brand-voice'])),
});

// Tool Implementations
server.setRequestHandler(ListToolsRequestSchema, async () => {
  return {
    tools: [
      {
        name: 'evaluate_content',
        description: 'Evaluate content quality against multiple metrics including readability, engagement potential, brand voice consistency, and platform optimization. Returns detailed scoring and improvement suggestions.',
        inputSchema: {
          type: 'object',
          properties: {
            content: { type: 'string', description: 'Content to evaluate' },
            contentType: { type: 'string', enum: ['twitter', 'linkedin', 'instagram', 'tiktok', 'farcaster', 'blog', 'email', 'video'], description: 'Type of content' },
            platform: { type: 'string', description: 'Target platform' },
            brandVoice: {
              type: 'object',
              properties: {
                tone: { type: 'string', enum: ['professional', 'casual', 'authoritative', 'friendly', 'provocative'] },
                personality: { type: 'array', items: { type: 'string' } },
                keywords: { type: 'array', items: { type: 'string' } },
                avoidKeywords: { type: 'array', items: { type: 'string' } },
              },
            },
            context: {
              type: 'object',
              properties: {
                workflowId: { type: 'string' },
                sessionId: { type: 'string' },
                projectId: { type: 'string' },
                source: { type: 'string', enum: ['claude-code', 'opencode', 'gemini', 'custom'] },
                timestamp: { type: 'string' },
              },
            },
            options: {
              type: 'object',
              properties: {
                includeSuggestions: { type: 'boolean' },
                strictMode: { type: 'boolean' },
                benchmarkAgainst: { type: 'string', enum: ['industry-average', 'top-performers', 'previous-outputs'] },
              },
            },
          },
          required: ['content', 'contentType'],
        },
      },
      {
        name: 'evaluate_hook',
        description: 'Evaluate hook effectiveness for social media content. Scores based on curiosity gap, specificity, emotional trigger, and platform-specific optimization.',
        inputSchema: {
          type: 'object',
          properties: {
            hook: { type: 'string', description: 'Hook to evaluate' },
            platform: { type: 'string', enum: ['twitter', 'linkedin', 'instagram', 'tiktok', 'farcaster'] },
            category: { type: 'string', enum: ['question', 'statement', 'contradiction', 'promise', 'story', 'controversy'] },
            context: {
              type: 'object',
              properties: {
                workflowId: { type: 'string' },
                sessionId: { type: 'string' },
                projectId: { type: 'string' },
                source: { type: 'string', enum: ['claude-code', 'opencode', 'gemini', 'custom'] },
              },
            },
          },
          required: ['hook', 'platform'],
        },
      },
      {
        name: 'track_performance',
        description: 'Track actual performance metrics for evaluated content. Enables the system to learn and improve predictions over time.',
        inputSchema: {
          type: 'object',
          properties: {
            evaluationId: { type: 'string', description: 'ID from previous evaluation' },
            actualMetrics: {
              type: 'object',
              properties: {
                impressions: { type: 'number' },
                engagement: { type: 'number' },
                shares: { type: 'number' },
                comments: { type: 'number' },
                saves: { type: 'number' },
                clicks: { type: 'number' },
                conversions: { type: 'number' },
                reach: { type: 'number' },
                followerGrowth: { type: 'number' },
              },
            },
            publishedAt: { type: 'string', description: 'ISO timestamp when content was published' },
            platform: { type: 'string', description: 'Platform where content was published' },
          },
          required: ['evaluationId', 'actualMetrics', 'publishedAt', 'platform'],
        },
      },
      {
        name: 'get_metrics',
        description: 'Retrieve aggregated metrics for content quality, engagement predictions, and improvement trends over time.',
        inputSchema: {
          type: 'object',
          properties: {
            timeRange: { type: 'string', enum: ['24h', '7d', '30d', '90d', 'all'] },
            contentType: { type: 'string' },
            platform: { type: 'string' },
            projectId: { type: 'string' },
            metricType: { type: 'string', enum: ['quality', 'engagement', 'improvement', 'all'] },
          },
        },
      },
      {
        name: 'get_audit_trail',
        description: 'Retrieve audit trail of all evaluations with filtering by session, project, source (ClaudeCode/OpenCode), and date range.',
        inputSchema: {
          type: 'object',
          properties: {
            sessionId: { type: 'string' },
            projectId: { type: 'string' },
            source: { type: 'string', enum: ['claude-code', 'opencode', 'gemini', 'all'] },
            startDate: { type: 'string' },
            endDate: { type: 'string' },
            limit: { type: 'number' },
          },
        },
      },
      {
        name: 'compare_content',
        description: 'Compare two versions of content across multiple metrics to identify improvements and regressions.',
        inputSchema: {
          type: 'object',
          properties: {
            contentA: { type: 'string', description: 'Original content' },
            contentB: { type: 'string', description: 'Revised content' },
            contentType: { type: 'string' },
            metrics: { type: 'array', items: { type: 'string', enum: ['quality', 'readability', 'engagement', 'seo', 'brand-voice'] } },
          },
          required: ['contentA', 'contentB', 'contentType', 'metrics'],
        },
      },
      {
        name: 'generate_improvements',
        description: 'Generate specific improvement suggestions for content based on evaluation results.',
        inputSchema: {
          type: 'object',
          properties: {
            content: { type: 'string' },
            evaluationId: { type: 'string' },
            focusArea: z.enum(['hook', 'body', 'cta', 'readability', 'engagement', 'seo', 'brand-voice', 'all']).default('all'),
            numberOfSuggestions: z.number().min(1).max(10).default(5),
          },
          required: ['content'],
        },
      },
    ],
  };
});

server.setRequestHandler(CallToolRequestSchema, async (request) => {
  const { name, arguments: args } = request.params;

  try {
    switch (name) {
      case 'evaluate_content': {
        const result = await evaluateContent(EvaluateContentSchema.parse(args));
        await logEvaluation({
          type: 'content-evaluation',
          source: args.context?.source || 'claude-code',
          sessionId: args.context?.sessionId,
          projectId: args.context?.projectId,
          workflowId: args.context?.workflowId,
          contentType: args.contentType,
          result,
        });
        return { content: [{ type: 'text', text: JSON.stringify(result, null, 2) }] };
      }

      case 'evaluate_hook': {
        const result = await evaluateHook(EvaluateHookSchema.parse(args));
        await logEvaluation({
          type: 'hook-evaluation',
          source: args.context?.source || 'claude-code',
          sessionId: args.context?.sessionId,
          projectId: args.context?.projectId,
          workflowId: args.context?.workflowId,
          contentType: args.platform,
          result,
        });
        return { content: [{ type: 'text', text: JSON.stringify(result, null, 2) }] };
      }

      case 'track_performance': {
        const result = await trackPerformance(TrackPerformanceSchema.parse(args));
        return { content: [{ type: 'text', text: JSON.stringify(result, null, 2) }] };
      }

      case 'get_metrics': {
        const result = await getMetrics(GetMetricsSchema.parse(args));
        return { content: [{ type: 'text', text: JSON.stringify(result, null, 2) }] };
      }

      case 'get_audit_trail': {
        const result = await getAuditTrail(GetAuditTrailSchema.parse(args));
        return { content: [{ type: 'text', text: JSON.stringify(result, null, 2) }] };
      }

      case 'compare_content': {
        const result = await compareContent(CompareContentSchema.parse(args));
        return { content: [{ type: 'text', text: JSON.stringify(result, null, 2) }] };
      }

      case 'generate_improvements': {
        const result = await generateImprovements(args);
        return { content: [{ type: 'text', text: JSON.stringify(result, null, 2) }] };
      }

      default:
        throw new Error(`Unknown tool: ${name}`);
    }
  } catch (error) {
    if (error instanceof z.ZodError) {
      throw new Error(`Invalid arguments: ${error.errors.map(e => e.message).join(', ')}`);
    }
    throw error;
  }
});

// Helper function for comparing content
async function compareContent(args: z.infer<typeof CompareContentSchema>) {
  const evalA = await evaluateContent({
    content: args.contentA,
    contentType: args.contentType as any,
    options: { includeSuggestions: false },
  });
  
  const evalB = await evaluateContent({
    content: args.contentB,
    contentType: args.contentType as any,
    options: { includeSuggestions: false },
  });

  const comparison: Record<string, any> = {};
  
  for (const metric of args.metrics) {
    const valueA = evalA.scores[metric as keyof typeof evalA.scores];
    const valueB = evalB.scores[metric as keyof typeof evalB.scores];
    const improvement = valueB - valueA;
    
    comparison[metric] = {
      original: valueA,
      revised: valueB,
      improvement,
      percentageChange: valueA > 0 ? ((improvement / valueA) * 100).toFixed(1) + '%' : 'N/A',
      winner: improvement > 0 ? 'revised' : improvement < 0 ? 'original' : 'tie',
    };
  }

  return {
    comparison,
    summary: Object.values(comparison).filter((c: any) => c.winner === 'revised').length > 
             Object.values(comparison).filter((c: any) => c.winner === 'original').length 
             ? 'revised' : 'original',
    timestamp: new Date().toISOString(),
  };
}

// Helper function for generating improvements
async function generateImprovements(args: any) {
  const content = args.content;
  const focusArea = args.focusArea || 'all';
  const numberOfSuggestions = args.numberOfSuggestions || 5;

  const evaluation = await evaluateContent({
    content,
    contentType: 'blog', // Default to blog for improvements
    options: { includeSuggestions: true },
  });

  let suggestions = evaluation.suggestions || [];
  
  if (focusArea !== 'all') {
    suggestions = suggestions.filter((s: any) => s.area === focusArea);
  }

  return {
    suggestions: suggestions.slice(0, numberOfSuggestions),
    priorityOrder: suggestions.map((s: any) => ({ area: s.area, impact: s.impact })),
    timestamp: new Date().toISOString(),
  };
}

// Start the server
const transport = new StdioServerTransport();
server.connect(transport).catch(console.error);

console.log('Agentic Creator OS Evaluator MCP Server running...');
